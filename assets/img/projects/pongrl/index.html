<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<p>I have tried baking a rudimentary RL environment and a agent recipe to learn more about the eco-system.<br>
I have made <a href="https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/pong.py" rel="external nofollow noopener" target="_blank">pong.py</a> a environment which one can host either locally (localhost) or on  0.0.0.0 (LAN).Allowing to communicate to <a href="https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/mainmodel.py" rel="external nofollow noopener" target="_blank">mainmodel.py</a> which has to be connected to the same host and the same port. <br>
I have used a simple socket connection to transfer data rather than a flask/django backend as they are based on it giving a advantage of speed of communication. <br>
both the scripts have debug mode which allows one to see in which state or stage they are in. <br></p>
<h2 id="output">Output</h2>
<p><img src="https://github.com/hex-plex/Pong-ReinforcementLearning/raw/master/trained.gif" alt="pong-agent"></p>
<hr>

<p>The one on the left is a RL agent trained for 3 days and the one on right is a hardcoded AI. This is noticible the they nearly have the same level of skills.</p>

<h2 id="requirements">Requirements</h2>
<p>This requires only few basic libraries it only runs on python3, rather a small update can make it compatible with python2.
The libraries</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pickle
numpy
cv2
pygame
<span class="c"># Below are mostly available preinstalled in any python distribution</span>
socket
BytesIO <span class="c"># from io in python3 or directly in python2</span>
_thread <span class="c"># for python 3 and thread in python2</span>
</code></pre></div></div>
<p>Which are quite rudimentary and neccessary part of the code.</p>

<h2 id="multithreading">Multithreading</h2>
<p>Here I have use many threads to assure that no data is missed while any other calculation or process is running on the main thread as there might be some unwanted lag between inputs.Also another thread on mainmodel exists which take inputs  from the user for pausing and resuming its learning session</p>
<h2 id="sockets">Sockets</h2>
<p>I have used sockets to send data between scripts rather than using a instance of model and the pong env on the same script well that would work just fine (or better removing all the lag of communication) , but my goal was to make it more appealing and to make me learn more real life skills that is if I train a agent with raspberry pi ( or any other weaker mobile device for that matter ) as a part of the environment or in the environment a more viable option than running both simulation and the RL agent would be to do the computation and send in actions to it. This also gave me a good understanding about data compression and network security ,though here there is none used as it just accepts the first connection to the env. <br>
<br>
with the use of socket one could manually control the environment with keyboard and also have a RL agent learning (half - Duplex).</p>

<h2 id="demo">Demo</h2>
<p>To launch the env with mostly default values just running <a href="https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/pong.py" rel="external nofollow noopener" target="_blank">pong.py</a> directly would start the server with default configs.
Its always recommended to use its instance, apply your configuration you want it to run and start the server.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pong</span> <span class="kn">import</span> <span class="n">Pong</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">Pong</span><span class="p">(</span> <span class="n">levelodiff</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1">## This is the level of the inbuilt AI that plays against you.
</span>                          <span class="c1">## Set it to 4 and chill out (really try it!!) the scale is 1-3
</span>            <span class="n">debug</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1">## If set to True it will print the stage the environment is in and info the data inputs and outputs
</span>            <span class="n">render</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1">## It would render the image that is sent to the model, This slows down the process so not a good idea to use it.
</span>            <span class="n">server</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1">## This is to set server mode on , if set false the environment will be no different from ordinary pong game.
</span>            <span class="n">host</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span>      <span class="c1">## This is to specify where you want to host it, "" maps to local host "0.0.0.0" doesnt really map
</span>                          <span class="c1">## to your ip address yet
</span>            <span class="n">port</span><span class="o">=</span><span class="mi">12345</span>    <span class="c1">## This your port no. of the connection.
</span>          <span class="p">)</span>               <span class="c1">## This is to just initiate the environment
</span>
<span class="n">env</span><span class="p">.</span><span class="nf">start</span><span class="p">()</span><span class="c1">## This starts the env ie., hosts itself as per the given parameter and waits for a connection in async while continuing the game
</span></code></pre></div></div>

<p>To launch the RL agent running the <a href="https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/mainmodel.py" rel="external nofollow noopener" target="_blank">mainmodel.py</a> directly will run it in default config. <br>
To customize to your config import and create an instance and run the client. <br></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">mainmodel</span> <span class="kn">import</span> <span class="n">PolicyGradient</span>

<span class="c1">## This uses two hidden layer to out-put 2 probabilities
</span><span class="n">agent</span> <span class="o">=</span> <span class="nc">PolicyGradient</span><span class="p">(</span> <span class="n">resume</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1">## This is usefull to continue training from previos checkpoint.
</span>                        <span class="n">render</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>   <span class="c1">## This will render the image got through the socket,useful if model is in another computer
</span>                        <span class="n">host</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>     <span class="c1">## If the value is None it trys to find a localhost , else specific host is to be provided as a str
</span>                        <span class="n">port</span><span class="o">=</span><span class="mi">12345</span><span class="p">,</span>    <span class="c1">## default value is 12345 set is as required.
</span>                        <span class="n">hiddenUnits</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="c1">## This no of hiddenUnits in first layer depending on the dimension of input.
</span>                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1">## This is to set batch size for batch reiforcement learning rather than using single episode.
</span>                        <span class="n">learningRate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="c1">## This is to set learning rate
</span>                        <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>    <span class="c1">## This is to set gamma or the discount
</span>                        <span class="n">decayRate</span><span class="o">=</span><span class="mf">0.99</span> <span class="c1">## This is decayRate for RMSprop
</span>                      <span class="p">)</span>                <span class="c1">## This creates an instance of PolicyGradient algorithm as a client_socket
</span>
<span class="n">agent</span><span class="p">.</span><span class="nf">start</span><span class="p">()</span>         <span class="c1">## This starts the agent ie., connects to the server and communicates and learns from its experience.
</span></code></pre></div></div>

<p>Running them should launch the env and agent would start leaning. There is a propriety control in mainmodel.py which takes input in its terminal as</p>
<ul>
  <li>‘p’    ==&gt; Pausing/Resuming the training process</li>
  <li>‘q’    ==&gt; Quit the training process , But this makes a Checkpoint as interrupt.p</li>
  <li>‘ping’ ==&gt; This pings the pong server 10 times and returns the average time it took to send and receive packets.
else with a better score of agent, the model is saved continuosly as save.p.
    <h2 id="fast-training">Fast-training</h2>
    <p>I have experimented the whole thing with networking sockets and asynchronous methods limiting the training process to human level speed for which I have uploaded the pong-fast-train.tar.gz</p>
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nb">tar</span> <span class="nt">-xvzf</span> pong-fast-train.tar.gz
<span class="nb">cd </span>pong-fast
python mainmodel.py
<span class="nb">mv </span>save.p ../save.p
</code></pre></div>    </div>
    <p>So this should train the model and save it into the main directory this should load the trained model in the main algo.</p>
  </li>
</ul>
</body></html>
