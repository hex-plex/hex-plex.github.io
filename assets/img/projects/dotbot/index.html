<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<p>This is fully ROS integrated robot, which can be used in various purposes our main use case was to test out multiagent algorithms. We used it to take part in the national level competition, FlipKart Grid 3.0 Robotics challenge 2021.</p>

<p>I have worked on building the platform and the integrating ROS with the following robot. The localization was done using a Apriltag_ros and a array of cameras. Also worked on implementing autonomously navigating in the environment and computing the optimal path while considering other robots in the arena.</p>

<p>The complete package including the controller and the planner can be found at <a href="https://github.com/Robotics-Club-IIT-BHU/Dot-Bot" rel="external nofollow noopener" target="_blank">Dot-Bot</a>.</p>

<h2 id="robot-control">Robot control</h2>

<p>The robots control architecture is a state-of-the-art implementation using a microprocessor and a microcontroller along with sensor fusion for position estimation with external cameras for deteciton of apriltags that denote the robot.</p>

<p>The exact control architecture is briefly given in the below image which later is elaborated in 3 bulletin points</p>

<p><img src="dotbot_control.png" alt=""></p>

<ul>
  <li>Communication: Communication between the robot and the remote PC is done through <a href="http://wiki.ros.org/ROS/Tutorials/MultipleMachines" rel="external nofollow noopener" target="_blank">Networking of multiple machines</a>. The communication between the microprocessor and the Microcontroller is done through UART at a baudrate of 2000000.</li>
  <li>Pose Estimation: Pose estimation is done through using Encoder and Imu fusion for odometry(i.e., odom to base frame). Then we use and external camera for ground truth (i.e., map to base frame).</li>
  <li>Controller: We have used a simple velocity based 3 wheel omni drive to control the robot. The omni drive node works at 100 hz on the raspberrypi and the micro controller controls individual motors to trace the given velocity this made it very easy to transfer from the simulation to real world.</li>
</ul>

<p>The whole implementation of the control architecture can be found at <a href="https://github.com/Robotics-Club-IIT-BHU/Swarm-Bot-Hardware" rel="external nofollow noopener" target="_blank">Swarm-bot-hardware</a>.</p>

<h2 id="simulation">Simulation</h2>

<p>We have setup the whole simulation for these robots as we have modelled the real robot. We have also added the 2DOF droppers that can be seen on top of each robot.</p>

<p>We have setup Rviz to visualize the robots both in simulation and hardware.
<img src="gazebo.png" alt=""></p>
<p align="center">
<i>An demo of the robot in gazebo</i>
</p>

<h2 id="flipkart-grid-challenge">Flipkart grid Challenge</h2>
<p>The main task was to drop packages using the most optimal path, We had used <a href="https://github.com/PathPlanning/Continuous-CBS" rel="external nofollow noopener" target="_blank">CBS</a> with minor changes to best suite our problem statement. We also used <a href="https://github.com/nobleo/tracking_pid" rel="external nofollow noopener" target="_blank">Tracking PID</a> for tracing these trajectories and making it more general and robust to be used with other algorithms and systems.</p>

<p><img src="arena.jpg" alt=""></p>
<p align="center">
<i>Example of the arena in which it drops the package</i>
</p>

<p>The robot is now a developmental platform for other multiagent research at Indian Institute of Technology, Varanasi
The robot is now about to have minor upgrades for it be used in a more real world use case by addition of lidars and better on board computation.</p>

<p>A preliminary result of the path planning algorithm with package dropping can be seen below.</p>

<iframe src="https://drive.google.com/file/d/1NhuapFy1Y12LVFoKoIaRP3BYm-ReoEHW/preview" width="700" height="500"></iframe>

</body></html>
