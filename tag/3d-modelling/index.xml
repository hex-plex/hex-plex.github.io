<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3D Modelling | Somnath Kumar</title>
    <link>https://hex-plex.github.io/tag/3d-modelling/</link>
      <atom:link href="https://hex-plex.github.io/tag/3d-modelling/index.xml" rel="self" type="application/rss+xml" />
    <description>3D Modelling</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© Somnath Sendhil Kumar `2021`</copyright><lastBuildDate>Mon, 04 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hex-plex.github.io/images/icon_hucffb9d42d7c693bd257413760c4b1fda_31806_512x512_fill_lanczos_center_2.png</url>
      <title>3D Modelling</title>
      <link>https://hex-plex.github.io/tag/3d-modelling/</link>
    </image>
    
    <item>
      <title>iOTA - Modular Bot</title>
      <link>https://hex-plex.github.io/project/gym-iota/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/gym-iota/</guid>
      <description>&lt;p&gt;This is an Open-AI gym environment developed with a modular bot platform named &amp;lsquo;&lt;strong&gt;iOTA&lt;/strong&gt;&amp;rsquo;. The motive of this gym is to allow us to test out and develop Algorithms for such a MultiAgent System. This is further used to learn heirarchial planning of such a MultiAgent systems to develop a generalized swarm behaviour in the robots (i.e., Colabortively working towards achieving a objective). This is an project that is being developed under the &lt;strong&gt;RoBoReG division of the Robotics Club, IIT Varanasi.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here the robot is designed in &lt;em&gt;SolidWorks&lt;/em&gt; and being Simulated in &lt;em&gt;pybullet&lt;/em&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/raw/master/media/Random.gif&#34;/&gt;&lt;/p&gt;&lt;br/&gt;
&lt;p align=&#34;center&#34;&gt;Here the robots where controlled to form into this specified constellation.&lt;/p&gt;
&lt;h2 id=&#34;iota&#34;&gt;IOTA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/raw/master/media/bRoll.gif&#34; width=300 align=&#34;right&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;This is a cost effective modular robot platform developed by us, This contains two docking plates enabling it to dock with other robots at those locations. This has got a very antique WW1 tank inspired designed which allows the bot too with enough traction when used with catpillar tracks.&lt;/p&gt;
&lt;p&gt;For more info on the bot hardware and designs please visit this page &lt;a href=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/blob/master/hardware_Designs/README.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://onedrive.live.com/view.aspx?resid=3B7945FE006E67D!4175&amp;amp;ithint=file%2cdocx&amp;amp;authkey=!AA_ziTqK6vYo80c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;
&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;To install the latest features one could clone and install like so.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Robotics-Club-IIT-BHU/gym-iOTA
cd gym-iOTA
pip install -e gym-iOTA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;else for stable releases. the below would work fine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gym-iOTA
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;depedencies&#34;&gt;Depedencies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;gym&lt;/li&gt;
&lt;li&gt;pybullet&lt;/li&gt;
&lt;li&gt;opencv-python&lt;/li&gt;
&lt;li&gt;Pillow&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;This environment can be accessed using gym api, and a small demo scipt is given below.&lt;/p&gt;
&lt;h4 id=&#34;demopy&#34;&gt;demo.py&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym
import gym_iOTA

env = gym.make(&#39;iOTA-v0&#39;,
                render=True,                  ## This runs the simulator in GUI mode
                no_of_modules=10,             ## This spawns so many no of robots
                no_of_clusters=10,            ## This is for subdividing the total no of robots in cluster for efficient accessing.
                arena=(2,2),                  ## This sets the dimension of the forseeable space for the system
                low_control=True,             ## This flag enables the low level control of the bot to the user.
                )
while True:
  action = np.ones((env.no_of_modules, 4))    ## Where for each row we have four velocities
  dock = np.zeros(
              (env.no_of_modules,
              env.no_of_modules))             ## This is the adjancy matrix storing all the docking relationships

  observation, reward, done, info = env.step(action, dock)

  observation                                 ## This is the coordinates+orientation of each bot
  env.render(mode=&#39;cluster&#39;)                  ## This would return the image and even render if was not in GUI mode earlier.

  if done:
    break
env.close()                                   ## simply removes all the docks and the bots and disconnects from the simulator.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;objective&#34;&gt;Objective&lt;/h2&gt;
&lt;p&gt;The objective of the environment is to use these robots to push the box from one side of the arena to the other. The below is a dry run with classical localization algorithm - &lt;strong&gt;Particle Swarm Optimization Algorithm&lt;/strong&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/raw/master/media/Objective.gif&#34;/&gt; &lt;/p&gt;&lt;br/&gt;
&lt;p align=&#34;center&#34;&gt;Here the swarm of bots try to localize the box in the 3D environment&lt;/p&gt;&lt;br/&gt;
&lt;p&gt;The Reward function is defined as the negetive of the distance between the position of the cube to the end point which is on the other side of the arena.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.codecogs.com/eqnedit.php?latex=\dpi{150}&amp;space;\bg_white&amp;space;\fn_phv&amp;space;\large&amp;space;R&amp;space;=&amp;space;-&amp;space;\sum&amp;space;_{axes}&amp;space;(Cube_{pos}&amp;space;-&amp;space;End_{pos})^{2}&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://latex.codecogs.com/png.latex?\dpi{150}&amp;space;\bg_white&amp;space;\fn_phv&amp;space;\large&amp;space;R&amp;space;=&amp;space;-&amp;space;\sum&amp;space;_{axes}&amp;space;(Cube_{pos}&amp;space;-&amp;space;End_{pos})^{2}&#34; title=&#34;\large R = - \sum _{axes} (Cube_{pos} - End_{pos})^{2}&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;info&#34;&gt;Info&lt;/h2&gt;
&lt;p&gt;This section contains details about the environment API&amp;rsquo;s and utils available in it for developement.&lt;/p&gt;
&lt;h4 id=&#34;low_control-boolean-&#34;&gt;&lt;code&gt;low_control&lt;/code&gt; &lt;em&gt;(Boolean)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;rather confusing term is present to allow the user to get low level control of individual bot that is the to set the desired velocities of each wheel touching the ground. else the environment would by itself control the robot and listen to the planner that plans trajectory to the given setpoint.&lt;/p&gt;
&lt;h4 id=&#34;envaction_space-gymbox-&#34;&gt;&lt;code&gt;env.action_space&lt;/code&gt; &lt;em&gt;(gym.Box)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;The action space varies if one chooses &lt;code&gt;low_control&lt;/code&gt; to be &lt;strong&gt;True&lt;/strong&gt; or &lt;strong&gt;False&lt;/strong&gt;, if the low_control is set True then action space is vector of velocities of 4 wheel for each robot.&lt;br/&gt;
i.e., action_space shape = (no_of_modules, 4) &lt;br/&gt;
for each row [right_forward_wheel, left_forward_wheel, right_back_wheel, left_back_wheel] of that bot.&lt;/p&gt;
&lt;h4 id=&#34;envdockmatrix-npndarray-&#34;&gt;&lt;code&gt;env.dockMatrix&lt;/code&gt; &lt;em&gt;(np.ndarray)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This contains all the joints or dockings that exist in the system, this a array containing if a bot is connected with another.&lt;br/&gt;
i.e., dockMatrix shape = (no_of_modules, no_of_modules)&lt;br/&gt;
So if &lt;strong&gt;env.dockMatrix[i][j]&lt;/strong&gt; =  0 then it means there is no joint or docking between robot number i and j. similarly if &lt;strong&gt;env.dockMatrix[i][j]&lt;/strong&gt; = 1 then it means these two robots are docked together.&lt;br/&gt;
&lt;strong&gt;note&lt;/strong&gt; : &lt;strong&gt;env.dockMatrix[i][j]&lt;/strong&gt; = &lt;strong&gt;env.dockMatrix[j][i]&lt;/strong&gt; are same as they can have only one constrain between them. And no self docking is possible i.e., &lt;strong&gt;env.dockMatrix[i][i]&lt;/strong&gt; = 1 is not possible.&lt;br/&gt;
We pass a similar matrix to &lt;a href=&#34;#envstep-gymenvstep-&#34;&gt;&lt;code&gt;env.step&lt;/code&gt;&lt;/a&gt; function where-in we denote new joints that we want to make.&lt;/p&gt;
&lt;h4 id=&#34;envobservation_space-gymbox-&#34;&gt;&lt;code&gt;env.observation_space&lt;/code&gt; &lt;em&gt;(gym.Box)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;The observation space is simply vector of position and orientation of each robot.&lt;br/&gt;
i.e., observation_space shape = (no_of_modules, 6)&lt;br/&gt;
for each row [x_coor, y_coor, z_coor, roll, pitch, yaw] of that bot.&lt;/p&gt;
&lt;h4 id=&#34;enviotas-class-iota-&#34;&gt;&lt;code&gt;env.iotas&lt;/code&gt; &lt;em&gt;(class iOTA)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This is a list of robots that have been spawned into the simulator and gives us individual control over each robot. A list API&amp;rsquo;s and variables are given &lt;a href=&#34;https://hex-plex.github.io/gym-iOTA/gym_iOTA/envs&#34;&gt;here&lt;/a&gt;. &lt;br/&gt;
The index of each robot in the list the robot number assigned to it which is the convection we use through out.&lt;/p&gt;
&lt;h4 id=&#34;envstep-gymenvstep-&#34;&gt;&lt;code&gt;env.step&lt;/code&gt; &lt;em&gt;(gym.Env.step)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This takes in two inputs one action being the &lt;a href=&#34;#envaction_space-gymbox-&#34;&gt;&lt;code&gt;env.action_space&lt;/code&gt;&lt;/a&gt; and a dock matrix similar to the &lt;a href=&#34;#envdockmatrix-npndarray-&#34;&gt;&lt;code&gt;env.dockingMatrix&lt;/code&gt;&lt;/a&gt; &lt;br/&gt;
it returns the usual &lt;br/&gt;
&lt;em&gt;&lt;code&gt;observation, reward, done, info&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;envrender-npndarray-&#34;&gt;&lt;code&gt;env.render&lt;/code&gt; &lt;em&gt;(np.ndarray)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This returns the image of the top view of the arena. The image is also rendered using PIL&amp;rsquo;s Image class if &lt;code&gt;mode = &#39;human&#39;&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;envclose-gymenvclose&#34;&gt;&lt;code&gt;env.close&lt;/code&gt; &lt;em&gt;(gym.Env.close)&lt;/em&gt;:&lt;/h4&gt;
&lt;p&gt;This simply removes all the joints and dockings and removes all the bodies and closes the connection to the physics server.&lt;/p&gt;
&lt;h2 id=&#34;the-team&#34;&gt;The Team&lt;/h2&gt;
&lt;table&gt;
 &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/hex-plex&#34;&gt;
    &lt;img src=&#34;https://avatars0.githubusercontent.com/u/56990337?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Somnath Sendhil Kumar &lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
    &lt;/td&gt;
    &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/surabhit-08&#34;&gt;
    &lt;img src=&#34;https://avatars3.githubusercontent.com/u/62366465?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Surabhit Gupta&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
	&lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/rtharungowda&#34;&gt;
    &lt;img src=&#34;https://avatars1.githubusercontent.com/u/55887709?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;R Tharun Gowda&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
	&lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/Kritika-Bansal&#34;&gt;
    &lt;img src=&#34;https://avatars2.githubusercontent.com/u/57754061?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kritika Bansal&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
&lt;/table&gt;
&lt;h2 id=&#34;mentors&#34;&gt;Mentors&lt;/h2&gt;
&lt;table&gt;
 &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/lok-i&#34;&gt;
    &lt;img src=&#34;https://avatars1.githubusercontent.com/u/54435909?s=460&amp;u=29af076049dab351b2e43621e9a433919bf50fb1&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Lokesh Krishna&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
    &lt;/td&gt;
    &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/NiranthS&#34;&gt;
    &lt;img src=&#34;https://media-exp1.licdn.com/dms/image/C4D03AQE0VSQ1pjwEJQ/profile-displayphoto-shrink_200_200/0/1597415223546?e=1616025600&amp;v=beta&amp;t=MLymy6q1n58MV2aL2l-13cnGJytixf5qnQV7HhZ4itE&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Niranth Sai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;[1]&lt;/a&gt;
&lt;strong&gt;Swarmbot-Hardware&lt;/strong&gt;: Mondada, F., Pettinaro, G.C., Guignard, A. et al. Swarm-Bot: A New Distributed Robotic Concept. Autonomous Robots 17, 193â€“221 (2004). doi: 10.1023/B:AURO.0000033972.50769.1c.&lt;br/&gt;
This contains the Idea behind Swarmbot which was the bot we are inspired by. &lt;a href=&#34;http://people.idsia.ch/~luca/swarmbot-hardware.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://www.youtube.com/watch?v=77SEQ-kj8PI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Video]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt;[2]&lt;/a&gt;
&lt;strong&gt;Self-reconfiguring modular robot.&lt;/strong&gt; This contains all the pre-existing works in Modular robots and information of their prototypes &lt;a href=&#34;https://en.wikipedia.org/wiki/Self-reconfiguring_modular_robot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Wikipedia]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt;[3]&lt;/a&gt;
&lt;strong&gt;SMORES-EP&lt;/strong&gt; : Researchers develop modular bots that combine to form a single flexible machine. This article contains few bot ideas we adopted &lt;a href=&#34;https://www.engadget.com/2019-07-23-modular-robots-configure-smores.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt;[4]&lt;/a&gt;
&lt;strong&gt;Modular Self-Reconfiguring Robots&lt;/strong&gt; Rus Robotics laboratory&amp;rsquo;s work in the field Modular Robots &lt;a href=&#34;https://groups.csail.mit.edu/drl/modular_robots/modular_robots.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[MIT]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;5&#34;&gt;[5]&lt;/a&gt;
&lt;strong&gt;An integrated system for perception-driven autonomy with modular robots&lt;/strong&gt; J. Daudelin, G. Jing, T. Tosun, et al. An integrated system for perception-driven autonomy with modular robots Science Robotics  31 Oct 2018: Vol. 3, Issue 23, eaat4983 DOI: 10.1126/scirobotics.aat4983 &lt;a href=&#34;https://robotics.sciencemag.org/content/3/23/eaat4983.full?ijkey=iBq7yW7Z8vmjE&amp;amp;keytype=ref&amp;amp;siteid=robotics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;6&#34;&gt;[6]&lt;/a&gt;
&lt;strong&gt;Symbrion&lt;/strong&gt; S. Kernbach, E. Meister, F. Schlauter, et al. Symbiotic robot organisms: Replicator and Symbrion projects January 2008 DOI: 10.1145/1774674.1774685 Conference: Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems. This is one such project from which we have been immensily inspired &lt;a href=&#34;https://www.researchgate.net/publication/234116421_Symbiotic_robot_organisms_Replicator_and_Symbrion_projects&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Paper]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;7&#34;&gt;[7]&lt;/a&gt;
&lt;strong&gt;Modular Bot Algorithm&lt;/strong&gt; H. Ahmadzadeh, E. Masehian, Modular robotic systems: Methods and algorithms for abstraction, planning, control, and synchronization, Volume 223, 2015, Pages 27-64, ISSN 0004-3702, doi: 10.1016/j.artint.2015.02.004. &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0004370215000260&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Paper]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;8&#34;&gt;[8]&lt;/a&gt;
&lt;strong&gt;KiloBots&lt;/strong&gt; by Self-Organizing Systems Research Group &lt;a href=&#34;https://ssr.seas.harvard.edu/kilobots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; This is the most simplest swarm system by which we were able to learn about the real problems that we would face in a MultiAgent System.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;9&#34;&gt;[9]&lt;/a&gt;
&lt;strong&gt;KiloBots-MARL&lt;/strong&gt; by Somnath Sendhil Kumar, This is a MultiAgent Reinforcement Learning solution presented for the Kilobots system &lt;a href=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Code]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;10&#34;&gt;[10]&lt;/a&gt;
&lt;strong&gt;pyBullet&lt;/strong&gt;, This is the simulator that we used to simulate our Robots physics in and built The enviroment on top of it. &lt;a href=&#34;https://pybullet.org/wordpress/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; &lt;a href=&#34;https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.2ye70wns7io3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Docs]&lt;/a&gt; &lt;a href=&#34;https://github.com/bulletphysics/bullet3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Code]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;11&#34;&gt;[11]&lt;/a&gt;
&lt;strong&gt;Open-AI gym&lt;/strong&gt;, This framework connected the simulators to API&amp;rsquo;s that are standard form in implementing Reinforcement Learning.
&lt;a href=&#34;https://gym.openai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; &lt;a href=&#34;https://gym.openai.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Docs]&lt;/a&gt; &lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Code]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;12&#34;&gt;[12]&lt;/a&gt;
&lt;strong&gt;Pybullet and OpenAI gym&lt;/strong&gt;, This is a tutorial which helps in implementing Pybullet as the backend with OpenAI gym, &lt;a href=&#34;https://www.etedal.net/2020/04/pybullet-panda.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;13&#34;&gt;[13]&lt;/a&gt;
&lt;strong&gt;SW to URDF&lt;/strong&gt;, This is the SolidWorks plugin that we used to convert our SolidWork Designs into importable URDFS which is provided by the ROS community. &lt;a href=&#34;http://wiki.ros.org/sw_urdf_exporter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hand Imitation</title>
      <link>https://hex-plex.github.io/project/hand-imitation/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/hand-imitation/</guid>
      <description>&lt;p&gt;RL-based learning for a robotic arm to imitate a given hand in a image feed
with &lt;a href=&#34;https://github.com/hex-plex/gym-handOfJustice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;handOfJustice&lt;/a&gt; as our environment&lt;/p&gt;
&lt;h2 id=&#34;to-setup&#34;&gt;To Setup&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;pip install gym-handOfJustice
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;else&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;git clone https://github.com/hex-plex/gym-handOfJustice
cd gym-handOfJustice
pip install -e .
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;to-train&#34;&gt;To Train&lt;/h2&gt;
&lt;p&gt;we used Actor Critic technique to update the a CNN
examples are RL-train.py and RL-Test.py&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In RL-train we have built the Actor and the critic model using tensorflow&lt;/li&gt;
&lt;li&gt;In RL-Test we have used stable-baselines SAC model with LnCnnpolicy policy&lt;/li&gt;
&lt;li&gt;Dataset which we used consisted of 50,000 images that meant content for 50,000 episodes.
to use the same it could be downloaded from the &lt;a href=&#34;https://drive.google.com/file/d/1YeJecxl8LDR_r3JAWfSbDP4X_klVQfrO/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;drive link&lt;/a&gt;&lt;br/&gt;
&amp;mdash;-or&amp;mdash;-&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;wget --no-check-certificate -r &#39;https://docs.google.com/uc?export=download&amp;amp;id=1YeJecxl8LDR_r3JAWfSbDP4X_klVQfrO&#39; -O dataset.7z
pacman -Sy p7zip-full  
# Or any package manager you like
7z e dataset.7z
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;training-metrics&#34;&gt;Training Metrics&lt;/h2&gt;
&lt;p&gt;The training was completed( only on ~50% of the dataset ) over a span 36 days. Special thanks to Center for Computing and Information Services, IIT (BHU) varanasi to provide the computational power i.e., the Compute Cluster. It ran for about 20,960 episodes making upto 20 Million steps. the following are the log of all the metrics.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Actor Loss&lt;/th&gt;
&lt;th&gt;Critic Loss&lt;/th&gt;
&lt;th&gt;Reward&lt;/th&gt;
&lt;th&gt;Cummulative Reward&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/Hand-Imitation/blob/master/log_images/actor_loss.jpg?raw=True&#34; alt=&#34;Actor_loss&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/Hand-Imitation/blob/master/log_images/critic_loss.jpg?raw=True&#34; alt=&#34;Critic_loss&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/Hand-Imitation/blob/master/log_images/reward.jpg?raw=True&#34; alt=&#34;Reward&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/Hand-Imitation/blob/master/log_images/eps_total_reward.jpg?raw=True&#34; alt=&#34;Cum_reward&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These results may look decent but we should keep in mind that what i have tried to do is a end to end model for a very complex and really having a few layers after a mobile net is surely not sufficient for learning the forward kinematics of a robotic arm and being able to estimate the pose of the arm in the image.&lt;/p&gt;
&lt;h2 id=&#34;output&#34;&gt;Output&lt;/h2&gt;
&lt;p&gt;These are the best result after training over a limited amount of time&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;
we have used clips from different versions of trained model and environment so there is a edit on these clips that in the gym-handOfJustice==0.0.6 a flip in the environment was added to make feel of the robotic hand more mirror like which can be spotted in the gif files&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/hex-plex/Hand-Imitation/blob/master/normal%26four_diff.gif?raw=true&#34; alt=&#34;Output-1&#34;&gt;
&lt;img src=&#34;https://github.com/hex-plex/Hand-Imitation/blob/master/3Pose.gif?raw=true&#34; alt=&#34;Output-2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-end&#34;&gt;The End&lt;/h2&gt;
&lt;p&gt;Thats all from our side&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/hex-plex/Hand-Imitation/blob/master/Thank_You.gif?raw=true&#34; height=&#34;50%&#34; width=&#34;50%&#34;&gt;&lt;/img&gt; &lt;/p&gt;
&lt;h2 id=&#34;the-team&#34;&gt;The Team&lt;/h2&gt;
&lt;table&gt;
 &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/hex-plex&#34;&gt;
    &lt;img src=&#34;https://avatars0.githubusercontent.com/u/56990337?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Somnath Sendhil Kumar &lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
    &lt;/td&gt;
    &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/numberbee7070&#34;&gt;
    &lt;img src=&#34;https://avatars3.githubusercontent.com/u/63304283?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yash Garg&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
	&lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/infini8-13&#34;&gt;
    &lt;img src=&#34;https://avatars2.githubusercontent.com/u/54203063?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;L N Saaswath&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
	&lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/AtuL-KumaR-00&#34;&gt;
    &lt;img src=&#34;https://avatars3.githubusercontent.com/u/64649440?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Atul Kumar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Light Gun</title>
      <link>https://hex-plex.github.io/project/light-gun/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/light-gun/</guid>
      <description>&lt;p&gt;Bored in the quarantine period, revisiting my old video games with light gun inspired an idea of light gun with a LCD display , I went on to build it mainly as a android package as it would be more accessable to anyone. Then ended up using a Raspberry Pi as a standalone device to enable it&amp;rsquo;s use accross many platforms and devices.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Explanation for the working can be found in &lt;a href=&#34;https://github.com/hex-plex/Light-Gun/blob/master/EXPLANATION.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EXPLANATION.md&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;glimpse&#34;&gt;Glimpse:&lt;/h2&gt;
&lt;p&gt;This is the glimpse of the prototype made with Raspberry pi on a toy gun with all the number-munching done on the Raspberry and transmites messages to the local server through bluetooth. The previous method of running the proccessing and the actions is documented in &lt;strong&gt;Win-Py&lt;/strong&gt; directory.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/Light-Gun/master/images/Complete_setup.jpg&#34; width=&#34;370&#34; align=&#34;right&#34; border=&#34;20&#34;&gt;&lt;/img&gt;
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p align=&#34;left&#34;&gt;This is a prototype of the light gun with a camera module attached onto a toy-gun and with a Raspberry pi 3 b+ (unneccessary I know) and a 7.4 V li-ion battery, regulated with a buck converter to which two switches are connected on the Raspi&#39;s interrupt pins.&lt;/p&gt;&lt;hr/&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/Light-Gun/master/images/display_setup.jpg&#34; width=&#34;370&#34; align=&#34;left&#34;&gt;&lt;/img&gt;
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
This is the display setup with 4 IR-LEDs attached to the corners of the display which makes it very easy for detection in a wide range of lighting conditions.&lt;hr/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;p&gt;And after Installation the main code using &lt;a href=&#34;https://github.com/hex-plex/Light-Gun/tree/master/Raspi-c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raspi&lt;/a&gt; version or &lt;a href=&#34;https://github.com/hex-plex/Light-Gun/tree/master/Win-py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Local&lt;/a&gt; version, continue using it according to the guide lines for the same which is present in their respective folders itself.&lt;/p&gt;
&lt;p&gt;The Modules then should create a virtual mouse and start following the edge vector of the camera module.
The following is a demonstration of it in action&amp;hellip;. (ðŸ˜… coming up shortly, as I am fixing few deviation errors.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1hIYBv8l2JCylOfQS9Cc5D5Hp50Ui55D-/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Demo Link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;p&gt;Any kind of Contribution is always welcomed. The rules for any Contribution for now is Non existent as I am the only one working on it and I have not really structurized the project so well as mostly in its prototyping stage, So its not possible to do a lot(Anyways feel free to report any errors or bugs ðŸ˜„ ), Any Idea for a better implementation is also welcomed.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This is for getting R and t matrix  &lt;a href=&#34;https://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is for adjusting focal length (approximation) &lt;a href=&#34;https://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is for getting accurate intrensic matrix &lt;a href=&#34;https://www.learnopencv.com/camera-calibration-using-opencv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cmake&lt;/li&gt;
&lt;li&gt;Bluez&lt;/li&gt;
&lt;li&gt;Raspberry pi interupts&lt;/li&gt;
&lt;li&gt;Asynchronous coding&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
