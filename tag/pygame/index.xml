<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PyGame | Somnath Kumar</title>
    <link>https://hex-plex.github.io/tag/pygame/</link>
      <atom:link href="https://hex-plex.github.io/tag/pygame/index.xml" rel="self" type="application/rss+xml" />
    <description>PyGame</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© Somnath Sendhil Kumar `2021`</copyright><lastBuildDate>Thu, 03 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hex-plex.github.io/images/icon_hucffb9d42d7c693bd257413760c4b1fda_31806_512x512_fill_lanczos_center_2.png</url>
      <title>PyGame</title>
      <link>https://hex-plex.github.io/tag/pygame/</link>
    </image>
    
    <item>
      <title>KiloBot MultiAgent Reinforcement Learning</title>
      <link>https://hex-plex.github.io/project/kilobot-multiagentrl/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/kilobot-multiagentrl/</guid>
      <description>&lt;p&gt;This is an experimentation to learn about Swarm Robotics with help of MultiAgent Reinforcement learning. We have used KiloBot as a platform as these are very simple in the actions space and have very high degree of symmetry. The Main inspiration of this project is this paper&lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;setting-up&#34;&gt;Setting Up&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hex-plex/KiloBot-MultiAgent-RL
cd KiloBot-MultiAgent-RL
pip install --upgrade absl-python \
                      tensorflow \
                      gym \
                      opencv-python \
                      tensorflow_probability \
                      keras \
                      pygame
pip install -e gym-kiloBot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should fetch and install the basics packages needed and should install the environment&lt;/p&gt;
&lt;h3 id=&#34;sample-of-environment&#34;&gt;Sample of environment&lt;/h3&gt;
&lt;p&gt;These envs are running on a constant &lt;strong&gt;Dummy-actions!!&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Env with Graph Objective&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;Env with Localization Objective&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/env_test_graph_compress.gif?raw=true&#34; alt=&#34;Output-1&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/env_test_localize_compress.gif?raw=true&#34; alt=&#34;Output-2&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python env-test.py ## This will help you check the functionality of the environement and should give the sample code to understand the apis as well.
python model-train.py \
        --headless=True \             ## for headless training default False
        --objective=&amp;quot;localization&amp;quot; \  ## defines the objective default is graph
        --modules=10 \                ## This defines the no of modules to be initialized default 10
        --time_steps=100000 \         ## This is the total no of steps the agent will take while learning
        --histRange=10 \              ## This is the no of mu values for the histograms
        --logdir=&amp;quot;logs&amp;quot; \             ## This specifies the log location for TensorBoard
        --checkpoints=&amp;quot;checkpoints&amp;quot;   ## This is for defining the location where the model is to be saved
        --load_checkpoint=&amp;quot;checkpoints/iter-500&amp;quot; ## This loads the specified iteration

python play-model.py \ ## This should load trained weights and show the performance
        --load_checkpoint=&amp;quot;checkpoints/graphs/10&amp;quot; \ ## loads this model
        --modules=10  \                             ## no of modules in the env
        --objective=&amp;quot;localization&amp;quot; \                ## Sets the objective function
        --time_steps=10000 \                        ## No of iterations to be run
        --histRange=10                              ## Same def as above
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;After a exhaustive amount of training on varies combinations of no_of_modules and task in hand I have obtained results of the following algorithm for these parameters.
I have used &lt;code&gt;[5, 10, 15, 20, 25, 50]&lt;/code&gt; number of modules to carryout the tests.&lt;/p&gt;
&lt;p&gt;One may find the Tensorboard log files here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/11NtimYoXOBGopIxziAojti0k1kfbaVBQ/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; log_kilobot [Drive Link] (403MB)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;and the Model Weights for each parameters here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/12qpbPIOrC-hLGVn2a8GETrkNL89bt8Dt/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; checkpoint_kilobot [Drive Link] (63.3MB)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;graph-problem&#34;&gt;Graph Problem&lt;/h3&gt;
&lt;p&gt;The below table is based on the number of modules used but for result tabulation I am considering
&lt;code&gt;[5, 10, 20, 50]&lt;/code&gt;
number of modules. To infer many insights of the algorithm in hand.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;b&gt;Please click on the image to zoom in&lt;/b&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Trained on &lt;strong&gt;\&lt;/strong&gt; Model Run on&lt;/th&gt;
&lt;th&gt;10&lt;/th&gt;
&lt;th&gt;20&lt;/th&gt;
&lt;th&gt;50&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/10trainon10.gif?raw=true&#34; alt=&#34;Success1&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/10trainon20.gif?raw=true&#34; alt=&#34;Success2&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/10trainon50.gif?raw=true&#34; alt=&#34;Success3&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/20trainon20.gif?raw=true&#34; alt=&#34;Random1&#34;&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/50trainon50.gif?raw=true&#34; alt=&#34;Random2&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
The policies trained on 20 and 50 module system is completely random in sense it just goes forward.
&lt;p&gt;We can see clearly the policy trained with 10 modules have yield a very good amount of coordination between each other that is the critic model taking in the input image of the system is able to specify the bots and guide them, Hence a very good amount of generalization is found when we use this model with system with different number of modules as can be seen with the 20 module system but the non stationarity of a higher order system makes it complex for the critic to reward or give baseline to the actor, As the&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The order of the reward changes drastically.&lt;/li&gt;
&lt;li&gt;The Reading from the histogram becomes Muddy.&lt;/li&gt;
&lt;li&gt;The correlation of a reward to action of a single agent reduces drastically.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Hence the Model trained with 10 modules is not able to generalize to the 50 module system, &lt;strong&gt;BUT&lt;/strong&gt; It looks more promising than the policy trained for the 50 module system itself, hence a curiculum learning based approach seems promising to solve such a transfer learning problem.&lt;/p&gt;
&lt;h4 id=&#34;plots-for-training-with-10-modules&#34;&gt;Plots for Training with 10 modules&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Actor Loss&lt;/th&gt;
&lt;th&gt;Critic Loss&lt;/th&gt;
&lt;th&gt;Reward&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/actor10.jpg?raw=true&#34; alt=&#34;Actor Loss&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/critic10.jpg?raw=true&#34; alt=&#34;Critic Loss&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/reward10.jpg?raw=true&#34; alt=&#34;Reward&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h4 id=&#34;for-5-modules&#34;&gt;For 5 modules&lt;/h4&gt;
&lt;p&gt;The system can&amp;rsquo;t comphensate for the negetive reward got by using fuel as the reward got for graphing is much smaller in order than it that it chooses to be stagnent.
&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/5trainon5.gif?raw=true&#34; alt=&#34;Failed&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;localization-problem&#34;&gt;Localization Problem&lt;/h3&gt;
&lt;p&gt;Here the task was much easier but the training didnt yield any satisfactory result as.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there were a lot of non stationarities and the fact the system was modelled based on a fashion which couldn&amp;rsquo;t extend more than a small system of at best 10 modules &lt;a href=&#34;1&#34;&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;And the fact that the reward of localization couldnt exceed the negetive reward for moving the bot that is the bots choose to stand still to save fuel over exploring in most of the senarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below are few runs with &lt;code&gt;[10, 25]&lt;/code&gt;
Number of modules.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Localization trained for small n(=10)&lt;/th&gt;
&lt;th&gt;Localization trained for big n(=25)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/15loc15.gif?raw=True&#34; alt=&#34;loc10&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL/blob/master/images/25loc25.gif?raw=True&#34; alt=&#34;loc25&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here it is clearly visible that because of randomness and the fact more number of can cover a larger space of the system exploration is much easier than being greedy to stay at a point and save fuel.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgment&#34;&gt;Acknowledgment&lt;/h2&gt;
&lt;p&gt;To Center for Computing and Information Services, IIT (BHU) varanasi to provide the computational power, i.e., The Super Computer on which I was able to train for over &lt;strong&gt;10 Million Timesteps&lt;/strong&gt; for about 1.5 months.&lt;/p&gt;
&lt;p&gt;For any other query feel free to reachout.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;[1]&lt;/a&gt;
&lt;strong&gt;Guided Deep Reinforcement Learning for Swarm Systems&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1709.06011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv:1709.06011v1]&lt;/a&gt; [cs.MA] 18 Sep 2017&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pong Reinforcement Learning</title>
      <link>https://hex-plex.github.io/project/pong-reinforcementlearning/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/pong-reinforcementlearning/</guid>
      <description>&lt;p&gt;I have tried baking a rudimentary RL environment and a agent recipe to learn more about the eco-system.&lt;br/&gt;
I have made &lt;a href=&#34;https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/pong.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pong.py&lt;/a&gt; a environment which one can host either locally (localhost) or on  0.0.0.0 (LAN).Allowing to communicate to &lt;a href=&#34;https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/mainmodel.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mainmodel.py&lt;/a&gt; which has to be connected to the same host and the same port. &lt;br/&gt;
I have used a simple socket connection to transfer data rather than a flask/django backend as they are based on it giving a advantage of speed of communication. &lt;br/&gt;
both the scripts have debug mode which allows one to see in which state or stage they are in. &lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;output&#34;&gt;Output&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/hex-plex/Pong-ReinforcementLearning/raw/master/trained.gif&#34; alt=&#34;pong-agent&#34;&gt;&lt;/p&gt;
&lt;hr/&gt;
The one on the left is a RL agent trained for 3 days and the one on right is a hardcoded AI. This is noticible the they nearly have the same level of skills.
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;This requires only few basic libraries it only runs on python3, rather a small update can make it compatible with python2.
The libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pickle
numpy
cv2
pygame
# Below are mostly available preinstalled in any python distribution
socket
BytesIO # from io in python3 or directly in python2
_thread # for python 3 and thread in python2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which are quite rudimentary and neccessary part of the code.&lt;/p&gt;
&lt;h2 id=&#34;multithreading&#34;&gt;Multithreading&lt;/h2&gt;
&lt;p&gt;Here I have use many threads to assure that no data is missed while any other calculation or process is running on the main thread as there might be some unwanted lag between inputs.Also another thread on mainmodel exists which take inputs  from the user for pausing and resuming its learning session&lt;/p&gt;
&lt;h2 id=&#34;sockets&#34;&gt;Sockets&lt;/h2&gt;
&lt;p&gt;I have used sockets to send data between scripts rather than using a instance of model and the pong env on the same script well that would work just fine (or better removing all the lag of communication) , but my goal was to make it more appealing and to make me learn more real life skills that is if I train a agent with raspberry pi ( or any other weaker mobile device for that matter ) as a part of the environment or in the environment a more viable option than running both simulation and the RL agent would be to do the computation and send in actions to it. This also gave me a good understanding about data compression and network security ,though here there is none used as it just accepts the first connection to the env. &lt;br/&gt;
&lt;br/&gt;
with the use of socket one could manually control the environment with keyboard and also have a RL agent learning (half - Duplex).&lt;/p&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;
&lt;p&gt;To launch the env with mostly default values just running &lt;a href=&#34;https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/pong.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pong.py&lt;/a&gt; directly would start the server with default configs.
Its always recommended to use its instance, apply your configuration you want it to run and start the server.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pong import Pong

env = Pong( levelodiff=1, ## This is the level of the inbuilt AI that plays against you.
                          ## Set it to 4 and chill out (really try it!!) the scale is 1-3
            debug=False,  ## If set to True it will print the stage the environment is in and info the data inputs and outputs
            render=False, ## It would render the image that is sent to the model, This slows down the process so not a good idea to use it.
            server=True,  ## This is to set server mode on , if set false the environment will be no different from ordinary pong game.
            host=&amp;quot;&amp;quot;,      ## This is to specify where you want to host it, &amp;quot;&amp;quot; maps to local host &amp;quot;0.0.0.0&amp;quot; doesnt really map
                          ## to your ip address yet
            port=12345    ## This your port no. of the connection.
          )               ## This is to just initiate the environment

env.start()## This starts the env ie., hosts itself as per the given parameter and waits for a connection in async while continuing the game
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To launch the RL agent running the &lt;a href=&#34;https://github.com/hex-plex/Pong-ReinforcementLearning/blob/master/mainmodel.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mainmodel.py&lt;/a&gt; directly will run it in default config. &lt;br/&gt;
To customize to your config import and create an instance and run the client. &lt;br/&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mainmodel import PolicyGradient

## This uses two hidden layer to out-put 2 probabilities
agent = PolicyGradient( resume=False,  ## This is usefull to continue training from previos checkpoint.
                        render=True,   ## This will render the image got through the socket,useful if model is in another computer
                        host=None,     ## If the value is None it trys to find a localhost , else specific host is to be provided as a str
                        port=12345,    ## default value is 12345 set is as required.
                        hiddenUnits=250, ## This no of hiddenUnits in first layer depending on the dimension of input.
                        batch_size=10, ## This is to set batch size for batch reiforcement learning rather than using single episode.
                        learningRate=1e-3, ## This is to set learning rate
                        gamma=0.99,    ## This is to set gamma or the discount
                        decayRate=0.99 ## This is decayRate for RMSprop
                      )                ## This creates an instance of PolicyGradient algorithm as a client_socket

agent.start()         ## This starts the agent ie., connects to the server and communicates and learns from its experience.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running them should launch the env and agent would start leaning. There is a propriety control in mainmodel.py which takes input in its terminal as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;p&amp;rsquo;    ==&amp;gt; Pausing/Resuming the training process&lt;/li&gt;
&lt;li&gt;&amp;lsquo;q&amp;rsquo;    ==&amp;gt; Quit the training process , But this makes a Checkpoint as interrupt.p&lt;/li&gt;
&lt;li&gt;&amp;lsquo;ping&amp;rsquo; ==&amp;gt; This pings the pong server 10 times and returns the average time it took to send and receive packets.
else with a better score of agent, the model is saved continuosly as save.p.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fast-training&#34;&gt;Fast-training&lt;/h2&gt;
&lt;p&gt;I have experimented the whole thing with networking sockets and asynchronous methods limiting the training process to human level speed for which I have uploaded the pong-fast-train.tar.gz&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tar -xvzf pong-fast-train.tar.gz
cd pong-fast
python mainmodel.py
mv save.p ../save.p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this should train the model and save it into the main directory this should load the trained model in the main algo.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
