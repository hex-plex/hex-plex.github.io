<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Embedded Systems | Somnath Kumar</title>
    <link>https://hex-plex.github.io/tag/embedded-systems/</link>
      <atom:link href="https://hex-plex.github.io/tag/embedded-systems/index.xml" rel="self" type="application/rss+xml" />
    <description>Embedded Systems</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© Somnath Sendhil Kumar `2021`</copyright><lastBuildDate>Mon, 04 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hex-plex.github.io/images/icon_hucffb9d42d7c693bd257413760c4b1fda_31806_512x512_fill_lanczos_center_2.png</url>
      <title>Embedded Systems</title>
      <link>https://hex-plex.github.io/tag/embedded-systems/</link>
    </image>
    
    <item>
      <title>iOTA - Modular Bot</title>
      <link>https://hex-plex.github.io/project/gym-iota/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/gym-iota/</guid>
      <description>&lt;p&gt;This is an Open-AI gym environment developed with a modular bot platform named &amp;lsquo;&lt;strong&gt;iOTA&lt;/strong&gt;&amp;rsquo;. The motive of this gym is to allow us to test out and develop Algorithms for such a MultiAgent System. This is further used to learn heirarchial planning of such a MultiAgent systems to develop a generalized swarm behaviour in the robots (i.e., Colabortively working towards achieving a objective). This is an project that is being developed under the &lt;strong&gt;RoBoReG division of the Robotics Club, IIT Varanasi.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here the robot is designed in &lt;em&gt;SolidWorks&lt;/em&gt; and being Simulated in &lt;em&gt;pybullet&lt;/em&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/raw/master/media/Random.gif&#34;/&gt;&lt;/p&gt;&lt;br/&gt;
&lt;p align=&#34;center&#34;&gt;Here the robots where controlled to form into this specified constellation.&lt;/p&gt;
&lt;h2 id=&#34;iota&#34;&gt;IOTA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/raw/master/media/bRoll.gif&#34; width=300 align=&#34;right&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;This is a cost effective modular robot platform developed by us, This contains two docking plates enabling it to dock with other robots at those locations. This has got a very antique WW1 tank inspired designed which allows the bot too with enough traction when used with catpillar tracks.&lt;/p&gt;
&lt;p&gt;For more info on the bot hardware and designs please visit this page &lt;a href=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/blob/master/hardware_Designs/README.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://onedrive.live.com/view.aspx?resid=3B7945FE006E67D!4175&amp;amp;ithint=file%2cdocx&amp;amp;authkey=!AA_ziTqK6vYo80c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;
&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;To install the latest features one could clone and install like so.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Robotics-Club-IIT-BHU/gym-iOTA
cd gym-iOTA
pip install -e gym-iOTA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;else for stable releases. the below would work fine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gym-iOTA
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;depedencies&#34;&gt;Depedencies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;gym&lt;/li&gt;
&lt;li&gt;pybullet&lt;/li&gt;
&lt;li&gt;opencv-python&lt;/li&gt;
&lt;li&gt;Pillow&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;This environment can be accessed using gym api, and a small demo scipt is given below.&lt;/p&gt;
&lt;h4 id=&#34;demopy&#34;&gt;demo.py&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym
import gym_iOTA

env = gym.make(&#39;iOTA-v0&#39;,
                render=True,                  ## This runs the simulator in GUI mode
                no_of_modules=10,             ## This spawns so many no of robots
                no_of_clusters=10,            ## This is for subdividing the total no of robots in cluster for efficient accessing.
                arena=(2,2),                  ## This sets the dimension of the forseeable space for the system
                low_control=True,             ## This flag enables the low level control of the bot to the user.
                )
while True:
  action = np.ones((env.no_of_modules, 4))    ## Where for each row we have four velocities
  dock = np.zeros(
              (env.no_of_modules,
              env.no_of_modules))             ## This is the adjancy matrix storing all the docking relationships

  observation, reward, done, info = env.step(action, dock)

  observation                                 ## This is the coordinates+orientation of each bot
  env.render(mode=&#39;cluster&#39;)                  ## This would return the image and even render if was not in GUI mode earlier.

  if done:
    break
env.close()                                   ## simply removes all the docks and the bots and disconnects from the simulator.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;objective&#34;&gt;Objective&lt;/h2&gt;
&lt;p&gt;The objective of the environment is to use these robots to push the box from one side of the arena to the other. The below is a dry run with classical localization algorithm - &lt;strong&gt;Particle Swarm Optimization Algorithm&lt;/strong&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/Robotics-Club-IIT-BHU/gym-iOTA/raw/master/media/Objective.gif&#34;/&gt; &lt;/p&gt;&lt;br/&gt;
&lt;p align=&#34;center&#34;&gt;Here the swarm of bots try to localize the box in the 3D environment&lt;/p&gt;&lt;br/&gt;
&lt;p&gt;The Reward function is defined as the negetive of the distance between the position of the cube to the end point which is on the other side of the arena.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.codecogs.com/eqnedit.php?latex=\dpi{150}&amp;space;\bg_white&amp;space;\fn_phv&amp;space;\large&amp;space;R&amp;space;=&amp;space;-&amp;space;\sum&amp;space;_{axes}&amp;space;(Cube_{pos}&amp;space;-&amp;space;End_{pos})^{2}&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://latex.codecogs.com/png.latex?\dpi{150}&amp;space;\bg_white&amp;space;\fn_phv&amp;space;\large&amp;space;R&amp;space;=&amp;space;-&amp;space;\sum&amp;space;_{axes}&amp;space;(Cube_{pos}&amp;space;-&amp;space;End_{pos})^{2}&#34; title=&#34;\large R = - \sum _{axes} (Cube_{pos} - End_{pos})^{2}&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;info&#34;&gt;Info&lt;/h2&gt;
&lt;p&gt;This section contains details about the environment API&amp;rsquo;s and utils available in it for developement.&lt;/p&gt;
&lt;h4 id=&#34;low_control-boolean-&#34;&gt;&lt;code&gt;low_control&lt;/code&gt; &lt;em&gt;(Boolean)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;rather confusing term is present to allow the user to get low level control of individual bot that is the to set the desired velocities of each wheel touching the ground. else the environment would by itself control the robot and listen to the planner that plans trajectory to the given setpoint.&lt;/p&gt;
&lt;h4 id=&#34;envaction_space-gymbox-&#34;&gt;&lt;code&gt;env.action_space&lt;/code&gt; &lt;em&gt;(gym.Box)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;The action space varies if one chooses &lt;code&gt;low_control&lt;/code&gt; to be &lt;strong&gt;True&lt;/strong&gt; or &lt;strong&gt;False&lt;/strong&gt;, if the low_control is set True then action space is vector of velocities of 4 wheel for each robot.&lt;br/&gt;
i.e., action_space shape = (no_of_modules, 4) &lt;br/&gt;
for each row [right_forward_wheel, left_forward_wheel, right_back_wheel, left_back_wheel] of that bot.&lt;/p&gt;
&lt;h4 id=&#34;envdockmatrix-npndarray-&#34;&gt;&lt;code&gt;env.dockMatrix&lt;/code&gt; &lt;em&gt;(np.ndarray)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This contains all the joints or dockings that exist in the system, this a array containing if a bot is connected with another.&lt;br/&gt;
i.e., dockMatrix shape = (no_of_modules, no_of_modules)&lt;br/&gt;
So if &lt;strong&gt;env.dockMatrix[i][j]&lt;/strong&gt; =  0 then it means there is no joint or docking between robot number i and j. similarly if &lt;strong&gt;env.dockMatrix[i][j]&lt;/strong&gt; = 1 then it means these two robots are docked together.&lt;br/&gt;
&lt;strong&gt;note&lt;/strong&gt; : &lt;strong&gt;env.dockMatrix[i][j]&lt;/strong&gt; = &lt;strong&gt;env.dockMatrix[j][i]&lt;/strong&gt; are same as they can have only one constrain between them. And no self docking is possible i.e., &lt;strong&gt;env.dockMatrix[i][i]&lt;/strong&gt; = 1 is not possible.&lt;br/&gt;
We pass a similar matrix to &lt;a href=&#34;#envstep-gymenvstep-&#34;&gt;&lt;code&gt;env.step&lt;/code&gt;&lt;/a&gt; function where-in we denote new joints that we want to make.&lt;/p&gt;
&lt;h4 id=&#34;envobservation_space-gymbox-&#34;&gt;&lt;code&gt;env.observation_space&lt;/code&gt; &lt;em&gt;(gym.Box)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;The observation space is simply vector of position and orientation of each robot.&lt;br/&gt;
i.e., observation_space shape = (no_of_modules, 6)&lt;br/&gt;
for each row [x_coor, y_coor, z_coor, roll, pitch, yaw] of that bot.&lt;/p&gt;
&lt;h4 id=&#34;enviotas-class-iota-&#34;&gt;&lt;code&gt;env.iotas&lt;/code&gt; &lt;em&gt;(class iOTA)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This is a list of robots that have been spawned into the simulator and gives us individual control over each robot. A list API&amp;rsquo;s and variables are given &lt;a href=&#34;https://hex-plex.github.io/gym-iOTA/gym_iOTA/envs&#34;&gt;here&lt;/a&gt;. &lt;br/&gt;
The index of each robot in the list the robot number assigned to it which is the convection we use through out.&lt;/p&gt;
&lt;h4 id=&#34;envstep-gymenvstep-&#34;&gt;&lt;code&gt;env.step&lt;/code&gt; &lt;em&gt;(gym.Env.step)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This takes in two inputs one action being the &lt;a href=&#34;#envaction_space-gymbox-&#34;&gt;&lt;code&gt;env.action_space&lt;/code&gt;&lt;/a&gt; and a dock matrix similar to the &lt;a href=&#34;#envdockmatrix-npndarray-&#34;&gt;&lt;code&gt;env.dockingMatrix&lt;/code&gt;&lt;/a&gt; &lt;br/&gt;
it returns the usual &lt;br/&gt;
&lt;em&gt;&lt;code&gt;observation, reward, done, info&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;envrender-npndarray-&#34;&gt;&lt;code&gt;env.render&lt;/code&gt; &lt;em&gt;(np.ndarray)&lt;/em&gt; :&lt;/h4&gt;
&lt;p&gt;This returns the image of the top view of the arena. The image is also rendered using PIL&amp;rsquo;s Image class if &lt;code&gt;mode = &#39;human&#39;&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;envclose-gymenvclose&#34;&gt;&lt;code&gt;env.close&lt;/code&gt; &lt;em&gt;(gym.Env.close)&lt;/em&gt;:&lt;/h4&gt;
&lt;p&gt;This simply removes all the joints and dockings and removes all the bodies and closes the connection to the physics server.&lt;/p&gt;
&lt;h2 id=&#34;the-team&#34;&gt;The Team&lt;/h2&gt;
&lt;table&gt;
 &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/hex-plex&#34;&gt;
    &lt;img src=&#34;https://avatars0.githubusercontent.com/u/56990337?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Somnath Sendhil Kumar &lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
    &lt;/td&gt;
    &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/surabhit-08&#34;&gt;
    &lt;img src=&#34;https://avatars3.githubusercontent.com/u/62366465?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Surabhit Gupta&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
	&lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/rtharungowda&#34;&gt;
    &lt;img src=&#34;https://avatars1.githubusercontent.com/u/55887709?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;R Tharun Gowda&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
	&lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/Kritika-Bansal&#34;&gt;
    &lt;img src=&#34;https://avatars2.githubusercontent.com/u/57754061?s=460&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kritika Bansal&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
&lt;/table&gt;
&lt;h2 id=&#34;mentors&#34;&gt;Mentors&lt;/h2&gt;
&lt;table&gt;
 &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/lok-i&#34;&gt;
    &lt;img src=&#34;https://avatars1.githubusercontent.com/u/54435909?s=460&amp;u=29af076049dab351b2e43621e9a433919bf50fb1&amp;v=4&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Lokesh Krishna&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
    &lt;/td&gt;
    &lt;td align=&#34;center&#34;&gt;
     &lt;a href=&#34;https://github.com/NiranthS&#34;&gt;
    &lt;img src=&#34;https://media-exp1.licdn.com/dms/image/C4D03AQE0VSQ1pjwEJQ/profile-displayphoto-shrink_200_200/0/1597415223546?e=1616025600&amp;v=beta&amp;t=MLymy6q1n58MV2aL2l-13cnGJytixf5qnQV7HhZ4itE&#34; width=&#34;100px;&#34; alt=&#34;&#34;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Niranth Sai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;
	&lt;/td&gt;
&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;[1]&lt;/a&gt;
&lt;strong&gt;Swarmbot-Hardware&lt;/strong&gt;: Mondada, F., Pettinaro, G.C., Guignard, A. et al. Swarm-Bot: A New Distributed Robotic Concept. Autonomous Robots 17, 193â€“221 (2004). doi: 10.1023/B:AURO.0000033972.50769.1c.&lt;br/&gt;
This contains the Idea behind Swarmbot which was the bot we are inspired by. &lt;a href=&#34;http://people.idsia.ch/~luca/swarmbot-hardware.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://www.youtube.com/watch?v=77SEQ-kj8PI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Video]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt;[2]&lt;/a&gt;
&lt;strong&gt;Self-reconfiguring modular robot.&lt;/strong&gt; This contains all the pre-existing works in Modular robots and information of their prototypes &lt;a href=&#34;https://en.wikipedia.org/wiki/Self-reconfiguring_modular_robot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Wikipedia]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt;[3]&lt;/a&gt;
&lt;strong&gt;SMORES-EP&lt;/strong&gt; : Researchers develop modular bots that combine to form a single flexible machine. This article contains few bot ideas we adopted &lt;a href=&#34;https://www.engadget.com/2019-07-23-modular-robots-configure-smores.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt;[4]&lt;/a&gt;
&lt;strong&gt;Modular Self-Reconfiguring Robots&lt;/strong&gt; Rus Robotics laboratory&amp;rsquo;s work in the field Modular Robots &lt;a href=&#34;https://groups.csail.mit.edu/drl/modular_robots/modular_robots.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[MIT]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;5&#34;&gt;[5]&lt;/a&gt;
&lt;strong&gt;An integrated system for perception-driven autonomy with modular robots&lt;/strong&gt; J. Daudelin, G. Jing, T. Tosun, et al. An integrated system for perception-driven autonomy with modular robots Science Robotics  31 Oct 2018: Vol. 3, Issue 23, eaat4983 DOI: 10.1126/scirobotics.aat4983 &lt;a href=&#34;https://robotics.sciencemag.org/content/3/23/eaat4983.full?ijkey=iBq7yW7Z8vmjE&amp;amp;keytype=ref&amp;amp;siteid=robotics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;6&#34;&gt;[6]&lt;/a&gt;
&lt;strong&gt;Symbrion&lt;/strong&gt; S. Kernbach, E. Meister, F. Schlauter, et al. Symbiotic robot organisms: Replicator and Symbrion projects January 2008 DOI: 10.1145/1774674.1774685 Conference: Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems. This is one such project from which we have been immensily inspired &lt;a href=&#34;https://www.researchgate.net/publication/234116421_Symbiotic_robot_organisms_Replicator_and_Symbrion_projects&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Paper]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;7&#34;&gt;[7]&lt;/a&gt;
&lt;strong&gt;Modular Bot Algorithm&lt;/strong&gt; H. Ahmadzadeh, E. Masehian, Modular robotic systems: Methods and algorithms for abstraction, planning, control, and synchronization, Volume 223, 2015, Pages 27-64, ISSN 0004-3702, doi: 10.1016/j.artint.2015.02.004. &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0004370215000260&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Paper]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;8&#34;&gt;[8]&lt;/a&gt;
&lt;strong&gt;KiloBots&lt;/strong&gt; by Self-Organizing Systems Research Group &lt;a href=&#34;https://ssr.seas.harvard.edu/kilobots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; This is the most simplest swarm system by which we were able to learn about the real problems that we would face in a MultiAgent System.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;9&#34;&gt;[9]&lt;/a&gt;
&lt;strong&gt;KiloBots-MARL&lt;/strong&gt; by Somnath Sendhil Kumar, This is a MultiAgent Reinforcement Learning solution presented for the Kilobots system &lt;a href=&#34;https://github.com/hex-plex/KiloBot-MultiAgent-RL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Code]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;10&#34;&gt;[10]&lt;/a&gt;
&lt;strong&gt;pyBullet&lt;/strong&gt;, This is the simulator that we used to simulate our Robots physics in and built The enviroment on top of it. &lt;a href=&#34;https://pybullet.org/wordpress/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; &lt;a href=&#34;https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.2ye70wns7io3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Docs]&lt;/a&gt; &lt;a href=&#34;https://github.com/bulletphysics/bullet3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Code]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;11&#34;&gt;[11]&lt;/a&gt;
&lt;strong&gt;Open-AI gym&lt;/strong&gt;, This framework connected the simulators to API&amp;rsquo;s that are standard form in implementing Reinforcement Learning.
&lt;a href=&#34;https://gym.openai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt; &lt;a href=&#34;https://gym.openai.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Docs]&lt;/a&gt; &lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Code]&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;12&#34;&gt;[12]&lt;/a&gt;
&lt;strong&gt;Pybullet and OpenAI gym&lt;/strong&gt;, This is a tutorial which helps in implementing Pybullet as the backend with OpenAI gym, &lt;a href=&#34;https://www.etedal.net/2020/04/pybullet-panda.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;13&#34;&gt;[13]&lt;/a&gt;
&lt;strong&gt;SW to URDF&lt;/strong&gt;, This is the SolidWorks plugin that we used to convert our SolidWork Designs into importable URDFS which is provided by the ROS community. &lt;a href=&#34;http://wiki.ros.org/sw_urdf_exporter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Link]&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PauciBot</title>
      <link>https://hex-plex.github.io/project/paucibot/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/paucibot/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This Repository contains the hardware designs of the Two Wheeled ROS enabled robot capable of self - balancing and also able to run SLAM and Localize itself with just Odometry, Encoders on the wheel and the Camera used with Depth Maps&lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt; to get a partially accurate Depth information of the surronding. This is partially inspired from the Turtlebot&lt;a href=&#34;#2&#34;&gt;[2]&lt;/a&gt; but it must be kept in mind the algorithm has be able to model the two wheeled bot to even account for its self balancing action which was solved using accurate System Modelling&lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/PauciBot-Hardware/master/images/Paucibot.jpg&#34; alt=&#34;Bot in all its glory&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;design&#34;&gt;Design&lt;/h2&gt;
&lt;p&gt;The above is the bot in all its glory, I had built The Hardware before running simulations and testing out its stability as its quite an symmetrical and simple design. This surely resembles turtlebot but has a few changes. Basically the idea was to have a worthy replacement for a turtlebot to test out all the mapping and planning algorithms on it.&lt;/p&gt;
&lt;br/&gt;
&lt;p&gt;After the construction it was quite easy to make a 3D design for the simulators. My Weapon of choice Phobos&lt;a href=&#34;#4&#34;&gt;[4]&lt;/a&gt; and &lt;a href=&#34;https://www.blender.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blender&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below is a simple design of the bot in blender.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/PauciBot-Hardware/master/images/Blender_Modelling.png&#34; alt=&#34;Blender Design&#34;&gt;&lt;/p&gt;
&lt;p&gt;With some Phobos magics ðŸŽ‰ðŸŽ‰&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/PauciBot-Hardware/master/images/URDF_generation.png&#34; alt=&#34;Phobos&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can set the parameters required for generating URDF of the bot.&lt;/p&gt;
&lt;p&gt;Exporting it just makes the work run naturally with Gazebo, PyBullet and any other Simulator of choice. The below is a simple &lt;strong&gt;PID control on the Pitch&lt;/strong&gt; of the Bot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/PauciBot-Hardware/master/images/PyBullet-PID.png&#34; alt=&#34;PyBullet PID&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can get the same output running the following in the terminal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hex-plex/PauciBot-Hardware.git
cd PauciBot-Hardware
python scripts/trial.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;under-development&#34;&gt;Under-Development&lt;/h2&gt;
&lt;p&gt;The Main objective was that I wanted to explore the Difficulties faced when taking the Project from simulation to reality and vice versa. The Main tool is &lt;strong&gt;ROS&lt;/strong&gt; as its a well established community with most useful tools opensourced for all. Hence I would like to release The bot with a ROS package. I have been currently working on the same for the past few months.&lt;/p&gt;
&lt;p&gt;Hope to see you there soon. : )&lt;/p&gt;
&lt;h2 id=&#34;components&#34;&gt;Components&lt;/h2&gt;
&lt;p&gt;The whole body is made of Ply wood and solid wood. And the wheels are just off the shelf RC car wheels with a 11.1 V LiPo battery.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4 id=&#34;arduino-mega-2560&#34;&gt;Arduino Mega 2560&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have used this for controlling the motor drivers and getting a Odometry and runnning a baseline self balancing algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4 id=&#34;nvidia-jetson-nano-4gb&#34;&gt;Nvidia Jetson Nano 4GB&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the ROS hardware Node on the Robot which communicates to a master on the localhost using UDP protocol. This is also used to give the neccesary computational power for onboard Computer Vision algorithms.
&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/PauciBot-Hardware/master/images/Jetson.jpg&#34; alt=&#34;Jetson&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4 id=&#34;mpu9250&#34;&gt;MPU9250&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simple sensor for Odometry.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4 id=&#34;l298-motor-driver&#34;&gt;L298 Motor Driver&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Motor driver for controlling 2 Motors&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4 id=&#34;lm2596s-buck&#34;&gt;LM2596S Buck&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For regulating voltage comming from the battery.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4 id=&#34;logitech-c290&#34;&gt;Logitech C290&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a high quality webcam that can be easily used for capturing images at incredibly high resolution and quality.
&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/PauciBot-Hardware/master/images/Logi_C290.jpg&#34; alt=&#34;Camera&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a id=&#34;1&#34;&gt;[1]&lt;/a&gt; Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints. Reza Mahjourian, Martin Wicke, Anelia Angelova &lt;a href=&#34;https://sites.google.com/view/vid2depth&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;2&#34;&gt;[2]&lt;/a&gt; Turtlebot is a ROS standard platform robot. &lt;a href=&#34;https://emanual.robotis.com/docs/en/platform/turtlebot3/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;3&#34;&gt;[3]&lt;/a&gt;
A Tutorial on Modelling and Control of Two- Wheeled Self-Balancing Robot with Stepper Motor &lt;a href=&#34;https://www.researchgate.net/publication/334731253_A_Tutorial_on_Modelling_and_Control_of_Two-_Wheeled_Self-Balancing_Robot_with_Stepper_Motor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;4&#34;&gt;[4]&lt;/a&gt; Phobos is an add-on for Blender. &lt;a href=&#34;https://github.com/dfki-ric/phobos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Light Gun</title>
      <link>https://hex-plex.github.io/project/light-gun/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://hex-plex.github.io/project/light-gun/</guid>
      <description>&lt;p&gt;Bored in the quarantine period, revisiting my old video games with light gun inspired an idea of light gun with a LCD display , I went on to build it mainly as a android package as it would be more accessable to anyone. Then ended up using a Raspberry Pi as a standalone device to enable it&amp;rsquo;s use accross many platforms and devices.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Explanation for the working can be found in &lt;a href=&#34;https://github.com/hex-plex/Light-Gun/blob/master/EXPLANATION.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EXPLANATION.md&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;glimpse&#34;&gt;Glimpse:&lt;/h2&gt;
&lt;p&gt;This is the glimpse of the prototype made with Raspberry pi on a toy gun with all the number-munching done on the Raspberry and transmites messages to the local server through bluetooth. The previous method of running the proccessing and the actions is documented in &lt;strong&gt;Win-Py&lt;/strong&gt; directory.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/Light-Gun/master/images/Complete_setup.jpg&#34; width=&#34;370&#34; align=&#34;right&#34; border=&#34;20&#34;&gt;&lt;/img&gt;
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p align=&#34;left&#34;&gt;This is a prototype of the light gun with a camera module attached onto a toy-gun and with a Raspberry pi 3 b+ (unneccessary I know) and a 7.4 V li-ion battery, regulated with a buck converter to which two switches are connected on the Raspi&#39;s interrupt pins.&lt;/p&gt;&lt;hr/&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/hex-plex/Light-Gun/master/images/display_setup.jpg&#34; width=&#34;370&#34; align=&#34;left&#34;&gt;&lt;/img&gt;
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
This is the display setup with 4 IR-LEDs attached to the corners of the display which makes it very easy for detection in a wide range of lighting conditions.&lt;hr/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
&lt;p&gt;And after Installation the main code using &lt;a href=&#34;https://github.com/hex-plex/Light-Gun/tree/master/Raspi-c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raspi&lt;/a&gt; version or &lt;a href=&#34;https://github.com/hex-plex/Light-Gun/tree/master/Win-py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Local&lt;/a&gt; version, continue using it according to the guide lines for the same which is present in their respective folders itself.&lt;/p&gt;
&lt;p&gt;The Modules then should create a virtual mouse and start following the edge vector of the camera module.
The following is a demonstration of it in action&amp;hellip;. (ðŸ˜… coming up shortly, as I am fixing few deviation errors.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1hIYBv8l2JCylOfQS9Cc5D5Hp50Ui55D-/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Demo Link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;p&gt;Any kind of Contribution is always welcomed. The rules for any Contribution for now is Non existent as I am the only one working on it and I have not really structurized the project so well as mostly in its prototyping stage, So its not possible to do a lot(Anyways feel free to report any errors or bugs ðŸ˜„ ), Any Idea for a better implementation is also welcomed.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This is for getting R and t matrix  &lt;a href=&#34;https://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is for adjusting focal length (approximation) &lt;a href=&#34;https://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is for getting accurate intrensic matrix &lt;a href=&#34;https://www.learnopencv.com/camera-calibration-using-opencv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cmake&lt;/li&gt;
&lt;li&gt;Bluez&lt;/li&gt;
&lt;li&gt;Raspberry pi interupts&lt;/li&gt;
&lt;li&gt;Asynchronous coding&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
